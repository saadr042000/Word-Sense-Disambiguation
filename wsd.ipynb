{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCjaWSh1-F7_",
        "outputId": "eca7ee76-5404-4a96-8eaa-23a281f88c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "from transformers import BertTokenizer, AdamW, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "nltk.download('punkt_tab')  # This line downloads the necessary data\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.wsd import lesk\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.tag import pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "@author: jcheung\n",
        "\n",
        "Developed for Python 2. Automatically converted to Python 3; may result in bugs.\n",
        "'''\n",
        "import xml.etree.cElementTree as ET\n",
        "import codecs\n",
        "\n",
        "class WSDInstance:\n",
        "    def __init__(self, my_id, lemma, context, index, pos=None):\n",
        "        self.id = my_id\n",
        "        self.lemma = lemma\n",
        "        self.context = context\n",
        "        self.index = index\n",
        "        self.pos = pos  # Store POS information\n",
        "    def __str__(self):\n",
        "        return '%s\\t%s\\t%s\\t%d\\t%s' % (self.id, self.lemma, ' '.join(self.context), self.index, self.pos)\n",
        "\n",
        "def load_instances(f):\n",
        "    '''\n",
        "    Load two lists of cases to perform WSD on. The structure that is returned is a dict, where\n",
        "    the keys are the ids, and the values are instances of WSDInstance.\n",
        "    '''\n",
        "    tree = ET.parse(f)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    dev_instances = {}\n",
        "    test_instances = {}\n",
        "\n",
        "    for text in root:\n",
        "        if text.attrib['id'].startswith('d001'):\n",
        "            instances = dev_instances\n",
        "        else:\n",
        "            instances = test_instances\n",
        "        for sentence in text:\n",
        "            # print(sentence)\n",
        "            # construct sentence context\n",
        "            context = [to_ascii(el.attrib['lemma']) for el in sentence]\n",
        "            # print(context)\n",
        "            for i, el in enumerate(sentence):\n",
        "                if el.tag == 'instance':\n",
        "                    my_id = el.attrib['id']\n",
        "                    lemma = to_ascii(el.attrib['lemma'])\n",
        "                    pos = pos_to_wordnet(el.attrib['pos'])\n",
        "                    instances[my_id] = WSDInstance(my_id, lemma, context, i, pos)\n",
        "    return dev_instances, test_instances\n",
        "\n",
        "def load_key(f):\n",
        "    '''\n",
        "    Load the solutions as dicts.\n",
        "    Key is the id\n",
        "    Value is the list of correct sense keys.\n",
        "    '''\n",
        "    dev_key = {}\n",
        "    test_key = {}\n",
        "    for line in open(f):\n",
        "        if len(line) <= 1: continue\n",
        "        #print (line)\n",
        "        doc, my_id, sense_key = line.strip().split(' ', 2)\n",
        "        if doc == 'd001':\n",
        "            dev_key[my_id] = sense_key.split()\n",
        "        else:\n",
        "            test_key[my_id] = sense_key.split()\n",
        "    return dev_key, test_key\n",
        "\n",
        "def to_ascii(s):\n",
        "    # remove all non-ascii characters\n",
        "    return codecs.encode(s, 'ascii', 'ignore').decode()\n",
        "\n",
        "def pos_to_wordnet(tag):\n",
        "    \"\"\"\n",
        "    Convert POS tags from input data to WordNet POS tags.\n",
        "    \"\"\"\n",
        "    if tag.startswith('NN'):  # Noun tags\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('VB'):  # Verb tags\n",
        "        return wn.VERB\n",
        "    elif tag.startswith('JJ'):  # Adjective tags\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('RB'):  # Adverb tags\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return None  # Tags not used in WordNet synsets\n",
        "\n",
        "data_f = 'multilingual-all-words.en.xml'\n",
        "key_f = 'wordnet.en.key'\n",
        "dev_instances, test_instances = load_instances(data_f)\n",
        "dev_key, test_key = load_key(key_f)\n",
        "\n",
        "# IMPORTANT: keys contain fewer entries than the instances; need to remove them\n",
        "dev_instances = {k:v for (k,v) in dev_instances.items() if k in dev_key}\n",
        "test_instances = {k:v for (k,v) in test_instances.items() if k in test_key}\n",
        "\n",
        "# read to use here\n",
        "print(len(dev_instances)) # number of dev instances\n",
        "print(len(test_instances)) # number of test instances\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQT6aXJj-JnD",
        "outputId": "88f610f6-25f7-43e0-b089-dfc2af68a460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194\n",
            "1450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Preprocessing Technique"
      ],
      "metadata": {
        "id": "84Nhr3rhk6Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Preprocessing Variants === #\n",
        "def synset_to_sense_key(synset):\n",
        "    return synset.lemmas()[0].key()\n",
        "\n",
        "def preprocess_baseline(context):\n",
        "    \"\"\"\n",
        "    Baseline preprocessing: lemmatization + stopword removal.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word in word_tokenize(\" \".join(context))\n",
        "        if word.lower() not in stop_words and word.isalpha()\n",
        "    ]\n",
        "\n",
        "def preprocess_pos_filtered(context):\n",
        "    \"\"\"\n",
        "    POS-filtered preprocessing: only include nouns, verbs, adjectives, and adverbs.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tags = pos_tag(context)\n",
        "    filtered_context = [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word, tag in pos_tags\n",
        "        if tag.startswith((\"N\", \"V\", \"J\", \"R\")) and word.isalpha()\n",
        "    ]\n",
        "    return filtered_context\n",
        "\n",
        "def preprocess_extended_context(context, lemma):\n",
        "    \"\"\"\n",
        "    Extended context preprocessing: include synonyms and hypernyms of context words.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    context = [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word in word_tokenize(\" \".join(context))\n",
        "        if word.lower() not in stop_words and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    synonyms, hypernyms = set(), set()\n",
        "    for word in context:\n",
        "        for synset in wn.synsets(word):\n",
        "            synonyms.update([lemma.name() for lemma in synset.lemmas()])\n",
        "            hypernyms.update([hypernym.name().split(\".\")[0] for hypernym in synset.hypernyms()])\n",
        "\n",
        "    return list(context) + list(synonyms) + list(hypernyms)\n",
        "\n",
        "# Adjusted Lesk Algorithm to handle multiple preprocessing variants\n",
        "def run_lesk_with_preprocessing(preprocessing_function, instances, gold_key):\n",
        "    \"\"\"\n",
        "    Run Lesk with the given preprocessing function.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "    for my_id, instance in instances.items():\n",
        "        # Handle preprocessing functions with multiple arguments\n",
        "        if preprocessing_function == preprocess_extended_context:\n",
        "            context = preprocessing_function(instance.context, instance.lemma)\n",
        "        else:\n",
        "            context = preprocessing_function(instance.context)\n",
        "\n",
        "        lesk_result = lesk(context, instance.lemma, instance.pos)\n",
        "        if lesk_result:\n",
        "            predictions[my_id] = synset_to_sense_key(lesk_result)\n",
        "    return predictions\n",
        "\n",
        "def evaluate_lesk_preprocessing(preprocessing_function, instances, gold_key):\n",
        "    \"\"\"\n",
        "    Evaluate Lesk with a specific preprocessing function.\n",
        "    \"\"\"\n",
        "    predictions = run_lesk_with_preprocessing(preprocessing_function, instances, gold_key)\n",
        "    gold_labels = [gold_key[k][0] for k in predictions.keys() if k in gold_key]\n",
        "    pred_labels = [predictions[k] for k in predictions.keys() if k in gold_key]\n",
        "    return accuracy_score(gold_labels, pred_labels)\n",
        "\n",
        "# === Run Experiments === #\n",
        "baseline_lesk_accuracy = evaluate_lesk_preprocessing(preprocess_baseline, dev_instances, dev_key)\n",
        "pos_filtered_lesk_accuracy = evaluate_lesk_preprocessing(preprocess_pos_filtered, dev_instances, dev_key)\n",
        "extended_context_lesk_accuracy = evaluate_lesk_preprocessing(preprocess_extended_context, dev_instances, dev_key)\n",
        "\n",
        "print(f\"Baseline Lesk Accuracy: {baseline_lesk_accuracy * 100:.2f}%\")\n",
        "print(f\"POS-Filtered Lesk Accuracy: {pos_filtered_lesk_accuracy * 100:.2f}%\")\n",
        "print(f\"Extended Context Lesk Accuracy: {extended_context_lesk_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uUU9mYabv6I",
        "outputId": "9da35b56-cdd8-4a4d-c242-f1951b8c30af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Lesk Accuracy: 25.26%\n",
            "POS-Filtered Lesk Accuracy: 25.77%\n",
            "Extended Context Lesk Accuracy: 25.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFS and Lesk"
      ],
      "metadata": {
        "id": "zLANFb7elAf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_context(context, lemma):\n",
        "    processed_context = preprocess_extended_context(context, lemma)\n",
        "    return processed_context\n",
        "\n",
        "def most_frequent_sense(lemma):\n",
        "    synsets = wn.synsets(lemma)\n",
        "    return synsets[0] if synsets else None\n",
        "\n",
        "\n",
        "def evaluate(predictions, gold_key):\n",
        "    gold_labels = [gold_key[k][0] for k in predictions.keys() if k in gold_key]\n",
        "    pred_labels = [predictions[k] for k in predictions.keys() if k in gold_key]\n",
        "    return accuracy_score(gold_labels, pred_labels)\n",
        "\n",
        "def run_mfs_baseline(instances, gold_key):\n",
        "    predictions = {}\n",
        "    for my_id, instance in instances.items():\n",
        "        mfs = most_frequent_sense(instance.lemma)\n",
        "        if mfs:\n",
        "            predictions[my_id] = synset_to_sense_key(mfs)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def run_lesk_algorithm(instances, gold_key):\n",
        "    predictions = {}\n",
        "    for my_id, instance in instances.items():\n",
        "        context = preprocess_context(instance.context, instance.lemma)\n",
        "        lesk_result = lesk(context, instance.lemma, instance.pos)\n",
        "        if lesk_result:\n",
        "            predictions[my_id] = synset_to_sense_key(lesk_result)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate Most Frequent Sense Baseline\n",
        "mfs_predictions = run_mfs_baseline(test_instances, test_key)\n",
        "# print first 5 entries  in mfs predictions\n",
        "print(list(mfs_predictions.items())[:5])\n",
        "print(list(test_key.items())[:5])\n",
        "# print first entry in train key\n",
        "\n",
        "mfs_accuracy = evaluate(mfs_predictions, test_key)\n",
        "print(f'Most Frequent Sense Baseline Accuracy: {mfs_accuracy * 100:.2f}%')\n",
        "\n",
        "# Evaluate Lesk's Algorithm\n",
        "lesk_predictions = run_lesk_algorithm(test_instances, test_key)\n",
        "lesk_accuracy = evaluate(lesk_predictions, test_key)\n",
        "print(f'Lesk Algorithm Accuracy: {lesk_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck0dy81Z-MSM",
        "outputId": "d0256a8c-96c1-4275-e283-e001ede6209f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('d002.s001.t001', 'victory%1:11:00::'), ('d002.s001.t002', 'israel%1:15:00::'), ('d002.s002.t001', 'victory%1:11:00::'), ('d002.s002.t002', 'visit%1:04:02::'), ('d002.s002.t005', 'team%1:14:00::')]\n",
            "[('d002.s001.t001', ['victory%1:11:00::']), ('d002.s001.t002', ['israel%1:15:00::']), ('d002.s002.t001', ['victory%1:11:00::']), ('d002.s002.t002', ['visit%1:04:02::']), ('d002.s002.t005', ['team%1:14:00::'])]\n",
            "Most Frequent Sense Baseline Accuracy: 49.17%\n",
            "Lesk Algorithm Accuracy: 29.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extended LESK"
      ],
      "metadata": {
        "id": "OdTxPaxClEBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_extended_lesk_algorithm(instances, gold_key):\n",
        "    predictions = {}\n",
        "    for my_id, instance in instances.items():\n",
        "        context = preprocess_context(instance.context, instance.lemma)\n",
        "        max_overlap = 0\n",
        "        best_synset = None\n",
        "\n",
        "        for synset in wn.synsets(instance.lemma, pos=instance.pos):\n",
        "            gloss = synset.definition().split()\n",
        "            examples = [word for ex in synset.examples() for word in ex.split()]\n",
        "            hypernyms = [hypernym.name().split(\".\")[0] for hypernym in synset.hypernyms()]\n",
        "            related_words = gloss + examples + hypernyms\n",
        "\n",
        "            overlap = len(set(context) & set(related_words))\n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                best_synset = synset\n",
        "\n",
        "        if best_synset:\n",
        "            predictions[my_id] = synset_to_sense_key(best_synset)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  # Extended Lesk\n",
        "lesk_predictions = run_extended_lesk_algorithm(test_instances, test_key)\n",
        "lesk_accuracy = evaluate(lesk_predictions, test_key)\n",
        "print(f\"Extended Lesk Algorithm Accuracy: {lesk_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_enhQeAVur1",
        "outputId": "2d60ccf0-c486-4a75-df34-257dad539cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extended Lesk Algorithm Accuracy: 35.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec and Hyperparameter Tuning on DEV set"
      ],
      "metadata": {
        "id": "2ePn0heelIRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# === Training Word2Vec Model === #\n",
        "# Use your dataset's contexts to train Word2Vec\n",
        "sentences = [instance.context for instance in dev_instances.values()]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def get_word_embedding(word, model):\n",
        "    \"\"\"\n",
        "    Get the Word2Vec embedding for a word.\n",
        "    \"\"\"\n",
        "    if word in model.wv:\n",
        "        return model.wv[word]\n",
        "    return np.zeros(model.vector_size)  # Return zero vector if word is not in vocabulary\n",
        "\n",
        "def get_context_embedding(context, model):\n",
        "    \"\"\"\n",
        "    Get the average Word2Vec embedding for a context.\n",
        "    \"\"\"\n",
        "    embeddings = [get_word_embedding(word, model) for word in context if word in model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    return np.zeros(model.vector_size)  # Return zero vector if no words in vocabulary\n",
        "\n",
        "# === Word2Vec-Based Prediction === #\n",
        "def word2vec_wsd(instance, model):\n",
        "    \"\"\"\n",
        "    Predict the word sense for a given instance using Word2Vec.\n",
        "    \"\"\"\n",
        "    context_embedding = get_context_embedding(instance.context, model)\n",
        "    sense_scores = {}\n",
        "\n",
        "    # Compute similarity between context and each sense definition\n",
        "    for synset in wn.synsets(instance.lemma):\n",
        "        definition_embedding = get_context_embedding(synset.definition().split(), model)\n",
        "        similarity = cosine_similarity([context_embedding], [definition_embedding])[0][0]\n",
        "        sense_scores[synset.lemmas()[0].key()] = similarity\n",
        "\n",
        "    # Return the sense with the highest similarity\n",
        "    return max(sense_scores, key=sense_scores.get) if sense_scores else None\n",
        "\n",
        "# === Evaluate Word2Vec Method === #\n",
        "def evaluate_word2vec_wsd(instances, gold_key, model):\n",
        "    predictions = {}\n",
        "    for my_id, instance in instances.items():\n",
        "        predicted_sense = word2vec_wsd(instance, model)\n",
        "        if predicted_sense:\n",
        "            predictions[my_id] = predicted_sense\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    gold_labels = [gold_key[k][0] for k in predictions.keys() if k in gold_key]\n",
        "    pred_labels = [predictions[k] for k in predictions.keys() if k in gold_key]\n",
        "    return accuracy_score(gold_labels, pred_labels)\n",
        "\n",
        "# Evaluate on the test set\n",
        "word2vec_accuracy = evaluate_word2vec_wsd(test_instances, test_key, word2vec_model)\n",
        "print(f\"Word2Vec WSD Accuracy: {word2vec_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hNRsXlh-2hx",
        "outputId": "bde91e7d-f067-4f20-866c-45c3b71334fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec WSD Accuracy: 27.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model2 = Word2Vec(sentences, vector_size=100, window=10, min_count=1, workers=4)\n",
        "word2vec_accuracy = evaluate_word2vec_wsd(test_instances, test_key, word2vec_model2)\n",
        "print(f\"Word2Vec WSD Accuracy: {word2vec_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h64Hq6jUctwn",
        "outputId": "cef73359-b914-4ae0-add1-426c1955c888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec WSD Accuracy: 27.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "_ButWZk3lQrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load pretrained BERT model and tokenizer, move model to GPU\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "def get_embedding(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Get the embedding for a given text using a pretrained BERT model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu()\n",
        "\n",
        "def bert_wsd(instance, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Perform WSD using BERT embeddings.\n",
        "    \"\"\"\n",
        "    context = ' '.join(instance.context)  # Combine the context words into a single string\n",
        "    context_embedding = get_embedding(context, model, tokenizer)\n",
        "    sense_scores = {}\n",
        "\n",
        "    # Use WordNet synsets to get senses and their definitions\n",
        "    for synset in wn.synsets(instance.lemma):\n",
        "        definition = synset.definition()\n",
        "        sense_embedding = get_embedding(definition, model, tokenizer)\n",
        "        similarity = 1 - cosine(context_embedding, sense_embedding)\n",
        "        sense_scores[synset.lemmas()[0].key()] = similarity\n",
        "\n",
        "    return max(sense_scores, key=sense_scores.get) if sense_scores else None\n",
        "\n",
        "def evaluate_bert_wsd(instances, gold_key, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluate the BERT-based WSD method on the dataset.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "    for my_id, instance in instances.items():\n",
        "        predicted_sense = bert_wsd(instance, model, tokenizer)\n",
        "        if predicted_sense:\n",
        "            predictions[my_id] = predicted_sense\n",
        "\n",
        "    # Debug: Print sample predictions\n",
        "    for my_id in list(predictions.keys())[5:10]:\n",
        "        print(f\"Instance ID: {my_id}\")\n",
        "        print(f\"Predicted: {predictions[my_id]}\")\n",
        "        print(f\"Gold: {gold_key.get(my_id, None)}\")\n",
        "        print()\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    gold_labels = [gold_key[k][0] for k in predictions.keys() if k in gold_key]\n",
        "    pred_labels = [predictions[k] for k in predictions.keys() if k in gold_key]\n",
        "    return accuracy_score(gold_labels, pred_labels)\n",
        "\n",
        "# Evaluate the BERT method on the test instances\n",
        "bert_accuracy = evaluate_bert_wsd(test_instances, test_key, bert_model, bert_tokenizer)\n",
        "print(f\"BERT WSD Accuracy on Test Set (GPU): {bert_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6x3upYvQ7Gw",
        "outputId": "ed7b3a00-0353-43ac-8a65-1ee77310b90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Instance ID: d002.s001.t001\n",
            "Predicted: victory%1:11:00::\n",
            "Gold: ['victory%1:11:00::']\n",
            "\n",
            "Instance ID: d002.s001.t002\n",
            "Predicted: israel%1:15:00::\n",
            "Gold: ['israel%1:15:00::']\n",
            "\n",
            "Instance ID: d002.s002.t001\n",
            "Predicted: victory%1:11:00::\n",
            "Gold: ['victory%1:11:00::']\n",
            "\n",
            "Instance ID: d002.s002.t002\n",
            "Predicted: visit%1:14:00::\n",
            "Gold: ['visit%1:04:02::']\n",
            "\n",
            "Instance ID: d002.s002.t005\n",
            "Predicted: team%1:14:01::\n",
            "Gold: ['team%1:14:00::']\n",
            "\n",
            "BERT WSD Accuracy on Test Set (GPU): 35.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device used: {device}\")\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "def compute_context_embedding(context, target_word):\n",
        "    \"\"\"\n",
        "    Compute contextualized embedding for a target word using BERT.\n",
        "    \"\"\"\n",
        "    # Prepare tokenized input from context\n",
        "    inputs = bert_tokenizer(' '.join(context), return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "\n",
        "    # Identify the index of the target word in the tokenized input\n",
        "    target_token_ids = bert_tokenizer.encode(target_word, add_special_tokens=False)\n",
        "    target_index = inputs['input_ids'][0].tolist().index(target_token_ids[0])\n",
        "\n",
        "    # Return the flattened contextualized embedding for the target word\n",
        "    return embeddings[0, target_index, :].view(-1).cpu().numpy()\n",
        "\n",
        "def compute_synset_embeddings(lemma):\n",
        "    \"\"\"\n",
        "    Generate embeddings for all synsets of a lemma using their definitions.\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    for synset in wn.synsets(lemma):\n",
        "        key = synset.lemmas()[0].key()\n",
        "        definition_embedding = compute_definition_embedding(synset.definition())\n",
        "        embeddings[key] = definition_embedding\n",
        "    return embeddings\n",
        "\n",
        "def compute_definition_embedding(text):\n",
        "    \"\"\"\n",
        "    Compute the embedding for a text (definition) using BERT.\n",
        "    \"\"\"\n",
        "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).view(-1).cpu().numpy()\n",
        "\n",
        "def predict_word_sense(context, target_word):\n",
        "    \"\"\"\n",
        "    Predict the sense of a target word based on the closest match to its contextualized embedding.\n",
        "    \"\"\"\n",
        "    # Compute the embedding for the target word in its context\n",
        "    target_embedding = compute_context_embedding(context, target_word)\n",
        "\n",
        "    # Compute the embeddings for the senses of the target word\n",
        "    sense_embeddings = compute_synset_embeddings(target_word)\n",
        "\n",
        "    # Compute cosine similarity and find the closest match\n",
        "    similarity_scores = {\n",
        "        sense_key: 1 - cosine(target_embedding, embedding)\n",
        "        for sense_key, embedding in sense_embeddings.items()\n",
        "    }\n",
        "    return max(similarity_scores, key=similarity_scores.get, default=None)\n",
        "\n",
        "def evaluate_model(instances, gold_key):\n",
        "    \"\"\"\n",
        "    Evaluate the WSD model on test instances.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "    for instance_id, instance in instances.items():\n",
        "        context = instance.context\n",
        "        target_word = instance.lemma\n",
        "        predicted_sense = predict_word_sense(context, target_word)\n",
        "        predictions[instance_id] = predicted_sense\n",
        "\n",
        "    gold_labels = [gold_key[k][0] for k in predictions if k in gold_key]\n",
        "    predicted_labels = [predictions[k] for k in predictions if k in gold_key]\n",
        "    return accuracy_score(gold_labels, predicted_labels)\n",
        "\n",
        "# Perform evaluation\n",
        "wsd_accuracy = evaluate_model(test_instances, test_key)\n",
        "print(f\"Word Sense Disambiguation Model Accuracy: {wsd_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "oNYh4axPin0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}